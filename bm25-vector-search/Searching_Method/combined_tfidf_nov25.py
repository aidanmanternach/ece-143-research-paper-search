# -*- coding: utf-8 -*-
"""Combined_TFIDF_Nov25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14qvlMqj3rosWVhxQ8jyASYn_jVou1d8E

##Imports
"""

import kagglehub
import json
import pandas as pd
import re
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import ipywidgets as widgets
from IPython.display import display, clear_output
from wordcloud import WordCloud
import networkx as nx
from itertools import combinations

"""##Data Loading"""

print("Downloading dataset...")
path = kagglehub.dataset_download("Cornell-University/arxiv")
print("Path to dataset files:", path)

original_dataset = path + "/arxiv-metadata-oai-snapshot.json"
subset_file = "arxiv_csAI_subset.json"
target_category = "cs.AI"

print(f"Creating subset for category: {target_category}...")
with open(original_dataset, "r") as infile, open(subset_file, "w") as outfile:
    for line in infile:
        data = json.loads(line)
        cats = data.get('categories', '')
        if target_category in cats:
            json.dump(data, outfile)
            outfile.write("\n")
print("Subset creation complete.")

def get_data_generator(file_path):
    with open(file_path) as f:
        for line in f:
            yield json.loads(line)

cols = ['id', 'authors', 'title', 'abstract', 'update_date', 'categories', 'doi']
data_gen = get_data_generator(subset_file)

df = pd.DataFrame([
    {col: paper.get(col) for col in cols}
    for paper in data_gen
])

df = df[df['abstract'].notna()].reset_index(drop=True)
print(f"Total papers with abstracts loaded: {len(df):,}")

"""##Text Pre-processing"""

def clean_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text) # Remove non-alphabetic characters
    text = ' '.join(text.split()) # Remove extra whitespace
    return text

df['title'] = df['title'].fillna('')
df['abstract'] = df['abstract'].fillna('')
df['combined_text'] = df['title'] + " " + df['abstract']

print("Preprocessing text...")
df['cleaned_text'] = df['combined_text'].apply(clean_text)
print("Text preprocessing complete.")

"""##TF-IDF model"""

print("Building TF-IDF model...")
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,
    stop_words='english',
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.8
)

tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])
print(f"TF-IDF Matrix Built: {tfidf_matrix.shape}")

"""#Global TF-IDF feature for the whole dataset"""

def plot_top_tfidf_features(vectorizer, matrix, top_n=20):
    """
    Plots the top N terms by their average TF-IDF score across all documents.
    This helps visualize which terms are most significant in the dataset.
    """
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt

    # Calculate average TF-IDF score for each term
    # mean(axis=0) returns a matrix, so we flatten it to a 1D array
    avg_scores = np.array(matrix.mean(axis=0)).flatten()

    # Get feature names from the vectorizer
    feature_names = vectorizer.get_feature_names_out()

    # Create a DataFrame
    features_df = pd.DataFrame({
        'term': feature_names,
        'score': avg_scores
    })

    # Sort and take top N
    top_features = features_df.sort_values(by='score', ascending=False).head(top_n)

    # Plot
    plt.figure(figsize=(10, 8))
    plt.barh(top_features['term'], top_features['score'], color='#2c3e50')
    plt.xlabel('Average TF-IDF Score')
    plt.ylabel('Term')
    plt.title(f'Top {top_n} Most Important Terms by TF-IDF Score')
    plt.gca().invert_yaxis()  # Highest score at the top
    plt.grid(axis='x', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

# Call the function (assuming tfidf_vectorizer and tfidf_matrix exist from Part 2)
plot_top_tfidf_features(tfidf_vectorizer, tfidf_matrix)

"""##Seardh Functionality"""

def search_papers(query, top_n=10):
    """
    Searches for papers using the global TF-IDF model.
    """
    cleaned_query = clean_text(query)
    query_vec = tfidf_vectorizer.transform([cleaned_query])

    # Calculate similarity
    cosine_scores = cosine_similarity(query_vec, tfidf_matrix).flatten()

    # Get top N results
    top_indices = cosine_scores.argsort()[-top_n:][::-1]

    # Retrieve results
    results = df.iloc[top_indices].copy()
    results['relevance_score'] = cosine_scores[top_indices]

    # Return relevant columns
    return results[['id', 'title', 'authors', 'update_date', 'relevance_score', 'combined_text']], cosine_scores

"""##Visualization Functions"""

def plot_relevance_bars(results, query):
    plt.figure(figsize=(10, 5))
    titles = [t[:50] + '...' if len(t) > 50 else t for t in results['title']]
    scores = results['relevance_score'].values

    plt.barh(range(len(titles)), scores, color=plt.cm.viridis(np.linspace(0.3, 0.9, len(titles))))
    plt.yticks(range(len(titles)), titles)
    plt.xlabel('Relevance Score')
    plt.title(f'Top Results for: "{query}"')
    plt.gca().invert_yaxis() # Best result at top
    plt.tight_layout()
    plt.show()

def plot_wordcloud(results):
    text = ' '.join(results['combined_text'])
    wc = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title('Common Terms in Search Results')
    plt.tight_layout()
    plt.show()

'''def plot_publication_trends(df_dataset):
    df_dataset['year'] = pd.to_datetime(df_dataset['update_date']).dt.year
    yearly_counts = df_dataset['year'].value_counts().sort_index()

    plt.figure(figsize=(10, 4))
    plt.plot(yearly_counts.index, yearly_counts.values, marker='o')
    plt.title('Publication Trends (cs.AI)')
    plt.xlabel('Year')
    plt.ylabel('Number of Papers')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()'''
    # --- Comparative Trend Analysis ---
import matplotlib.pyplot as plt
import json

# Define the categories we want to compare
categories_to_track = ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'cs.RO']
category_labels = {
    'cs.AI': 'Artificial Intelligence',
    'cs.LG': 'Machine Learning',
    'cs.CV': 'Computer Vision',
    'cs.CL': 'Computation & Language (NLP)',
    'cs.RO': 'Robotics'
}

# Initialize counters
# Structure: {'cs.AI': {2000: 50, 2001: 65...}, 'cs.LG': ...}
counts = {cat: {} for cat in categories_to_track}

print("Scanning full dataset to compare trends. This may take 1-2 minutes...")

# Scan the original file (Streaming, so no RAM crash)

with open(original_dataset, 'r') as f:
    for line in f:
        try:
            doc = json.loads(line)
            year = int(doc['update_date'][:4])

            # We only care about recent history (e.g., post-2000)
            if year < 2000: continue

            # Check if paper belongs to any of our target categories
            paper_categories = doc.get('categories', '').split()
            for cat in categories_to_track:
                if cat in paper_categories:
                    counts[cat][year] = counts[cat].get(year, 0) + 1
        except:
            continue

# Plot the comparison
plt.figure(figsize=(14, 7))

for cat in categories_to_track:
    # Sort data by year
    years = sorted(counts[cat].keys())
    paper_counts = [counts[cat][y] for y in years]

    plt.plot(years, paper_counts, marker='o', markersize=4, label=category_labels[cat], linewidth=2.5)

plt.title('The Explosion of AI: Comparing Sub-fields (2000-Present)', fontsize=16)
plt.xlabel('Year', fontsize=12)
plt.ylabel('Number of Papers Published', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, linestyle='--', alpha=0.5)
plt.xticks(range(2000, max(years)+1, 2)) # Show every other year
plt.tight_layout()
plt.show()

def explain_search_match(query, doc_text, vectorizer):
    """
    Visualizes WHY a specific document matched the query.
    It plots the contribution of each overlapping term to the final similarity score.
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np

    # 1. Vectorize the Query and the Document
    query_vec = vectorizer.transform([query])
    doc_vec = vectorizer.transform([doc_text])

    # 2. Calculate the Element-wise Product (The "Overlap")
    # This shows how much each word contributed to the Cosine Similarity
    # (Note: This ignores the denominator normalization for visualization simplicity,
    # but captures the relative weight perfectly)
    contribution = query_vec.multiply(doc_vec).toarray().flatten()

    # 3. specific indices where contribution > 0
    relevant_indices = contribution.nonzero()[0]

    if len(relevant_indices) == 0:
        print("No exact term overlap found (Score based on partial matches or empty).")
        return

    # 4. Map indices to words and scores
    feature_names = vectorizer.get_feature_names_out()
    words = [feature_names[i] for i in relevant_indices]
    scores = [contribution[i] for i in relevant_indices]

    # 5. Create DataFrame and Sort
    explanation_df = pd.DataFrame({'term': words, 'contribution': scores})
    explanation_df = explanation_df.sort_values(by='contribution', ascending=True) # Ascending for horizontal bar chart

    # 6. Plot
    plt.figure(figsize=(10, 4))
    # Color code: Darker blue for higher contribution
    plt.barh(explanation_df['term'], explanation_df['contribution'], color='#e74c3c')
    plt.xlabel('Contribution to Relevance Score')
    plt.title(f'Why is this the top result? (Term Contributions)')
    plt.grid(axis='x', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

# Example usage (you can test this in a new cell):
# explain_search_match("deep learning for vision", df['combined_text'].iloc[0], tfidf_vectorizer)

"""##Interactive Search Interface"""

print("\n--- Research Paper Search Engine ---")

# Widgets
query_input = widgets.Text(
    value='',
    placeholder='Enter keywords (e.g., "neural networks")',
    description='Query:',
    style={'description_width': 'initial'}
)

results_slider = widgets.IntSlider(
    value=5,
    min=1,
    max=20,
    description='Max Results:',
    style={'description_width': 'initial'}
)

search_button = widgets.Button(description="Search")
output_area = widgets.Output()

def on_search_clicked(b):
    with output_area:
        clear_output()
        query = query_input.value
        if not query:
            print("Please enter a search query.")
            return

        print(f"Searching for '{query}'...")
        results, _ = search_papers(query, top_n=results_slider.value)

        if results.empty or results['relevance_score'].max() == 0:
            print("No relevant papers found.")
        else:
            # Display Table
            display(results[['title', 'authors', 'update_date', 'relevance_score']])

            ##new code her 11/25##########################################
            top_paper_text = results.iloc[0]['combined_text']
            explain_search_match(query, top_paper_text, tfidf_vectorizer)

            # Display Visualizations
            plot_relevance_bars(results, query)
            plot_wordcloud(results)

search_button.on_click(on_search_clicked)

# Display UI
ui = widgets.VBox([
    widgets.HBox([query_input, results_slider, search_button]),
    output_area
])
display(ui)

"""##Network Analysis"""

!pip install pyvis

print("\n--- Author Collaboration Network (Top 50 Authors) ---")
# Helper to parse authors
def parse_authors(auth_str):
    if not isinstance(auth_str, str): return []
    return [a.strip() for a in auth_str.replace(' and ', ', ').split(',') if a.strip()]

df['author_list'] = df['authors'].apply(parse_authors)

# Build Graph
G = nx.Graph()
for authors in df['author_list']:
    if len(authors) > 1:
        for u, v in combinations(sorted(authors), 2):
            if G.has_edge(u, v):
                G[u][v]['weight'] += 1
            else:
                G.add_edge(u, v, weight=1)

# Analyze Top Authors
degree_cent = nx.degree_centrality(G)
top_authors = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:50]
top_author_names = [a[0] for a in top_authors]

subgraph = G.subgraph(top_author_names)

plt.figure(figsize=(10, 8))
pos = nx.spring_layout(subgraph, k=0.5)
nx.draw(subgraph, pos, with_labels=True, node_size=30, font_size=8,
        edge_color='gray', node_color='lightblue', alpha=0.7)
plt.title("Co-Authorship Network (Top 50 Central Authors)")
plt.show()

# Show Trends
# plot_publication_trends(df)

# --- Revised Part 6: Cleaner Network Analysis ---
print("\n--- Optimized Author Collaboration Network ---")

# 1. 重新建立完整的圖
G_full = nx.Graph()
for authors in df['author_list']:
    if len(authors) > 1:
        for u, v in combinations(sorted(authors), 2):
            if G_full.has_edge(u, v):
                G_full[u][v]['weight'] += 1
            else:
                G_full.add_edge(u, v, weight=1)

# 2. 關鍵改進：過濾「一次性合作」的邊 (Filter Weak Links)
# 只保留合作次數 >= 2 的關係。這能大幅減少雜訊！
edges_to_keep = [(u, v) for u, v, d in G_full.edges(data=True) if d['weight'] >= 2]
G_filtered = G_full.edge_subgraph(edges_to_keep)

# 3. 在過濾後的圖中，再選出最大的連通子圖或 Top 節點
# 這樣我們看到的都是「強關係」
degree_cent = nx.degree_centrality(G_filtered)
# 我們可以顯示多一點人，例如 Top 80，因為現在圖比較乾淨了
top_authors_filtered = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:80]
top_nodes = [a[0] for a in top_authors_filtered]

subgraph_clean = G_filtered.subgraph(top_nodes)

print(f"Nodes: {len(subgraph_clean.nodes())}, Edges: {len(subgraph_clean.edges())}")

# 4. 再次繪製互動圖 (使用之前的發光函式)
# plot_glowing_interactive_network(subgraph_clean, filename='clean_network.html')

# Pre-compute author categories to display in tooltips
author_to_categories_map = {}
for index, row in df.iterrows():
    # Ensure author_list is indeed a list of authors
    authors = row['author_list']
    # Ensure categories is a string and split it into a list
    paper_categories = row['categories'].split()

    for author in authors:
        if author not in author_to_categories_map:
            author_to_categories_map[author] = set()
        author_to_categories_map[author].update(paper_categories)

def plot_community_bridges_network(G, communities_detected, community_names_map, filename='community_bridge_network.html'):
    """
    Plots an interactive network with community detection and bridge author highlighting.
    Takes the graph, pre-detected communities, and descriptive community names as input.
    """
    from pyvis.network import Network
    from IPython.display import display, HTML
    import json

    # 1. Use pre-detected communities to build node_community_map
    communities = communities_detected

    # Print community sizes for clarity
    print(f"Detected {len(communities)} communities.")
    for i, comm in enumerate(communities):
        print(f"  Community {i}: {len(comm)} nodes")

    node_community_map = {}
    for i, comm in enumerate(communities):
        for node in comm:
            node_community_map[node] = i

    # 2. Identify Bridge Nodes
    bridge_nodes = []

    for node in G.nodes():
        my_comm = node_community_map.get(node)
        neighbors = list(G.neighbors(node))
        if not neighbors: continue

        foreign_neighbors = sum(1 for v in neighbors if node_community_map.get(v) != my_comm)

        if foreign_neighbors > 0:
            bridge_nodes.append(node)

    # 3. Set up PyVis visualization
    net = Network(notebook=True, height="650px", width="100%", bgcolor="#111111", font_color="white", cdn_resources='in_line')

    for node in G.nodes():
        comm_id = node_community_map.get(node)
        is_bridge = node in bridge_nodes

        node_color_config = None
        node_shape = 'dot'
        node_size = 15
        node_border_width = 1

        author_cats = sorted(list(author_to_categories_map.get(node, set())))
        categories_str = ", ".join(author_cats) if author_cats else "N/A"

        descriptive_community_name = community_names_map.get(comm_id, 'Undefined Community')

        title_text = f"Author: {node}\nCommunity: {descriptive_community_name}\nCategories: {categories_str}"
        if is_bridge:
            title_text += "\n(Cross-domain Bridge!)"

        if is_bridge:
            node_shape = 'diamond'
            node_size = 25
            node_border_width = 3
            node_color_config = {
                'border': '#FF0000',
                'highlight': {'border': '#FFD700'}
            }

        net.add_node(
            node,
            label=node,
            group=comm_id,
            title=title_text,
            font={'size': 14, 'color': 'white'},
            shape=node_shape,
            size=node_size,
            borderWidth=node_border_width,
            color=node_color_config
        )

    for source, target, data in G.edges(data=True):
        net.add_edge(source, target, value=data['weight'], color='#444444')

    # 4. Physics simulation settings (tuned for better stability with more nodes)
    physics_options = {
      "physics": {
        "solver": "forceAtlas2Based",
        "forceAtlas2Based": {
          "gravitationalConstant": -40000, # Increased negative value for stronger repulsion
          "centralGravity": 0.2,           # Slightly increased central gravity to prevent excessive spread
          "springLength": 120,             # Adjusted spring length for denser graphs
          "springConstant": 0.05,
          "avoidOverlap": 0.5              # Helps prevent node overlap
        },
        "stabilization": {
          "iterations": 3000,
          "fit": True,
          "enabled": True
        }
      }
    }
    net.set_options(json.dumps(physics_options))

    # 5. Save and display
    net.save_graph(filename)
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            display(HTML(f.read()))
    except Exception as e:
        print(f"Error: {e}")

# The direct call to plot_community_bridges_network here was for initial testing. It will be driven by the interactive widget now.
# print(f"Analyzing network... (Total {len(subgraph_clean.nodes())} nodes)")
# plot_community_bridges_network(subgraph_clean)

from networkx.algorithms import community

# Assuming subgraph_clean is the graph used for community detection
# (from the previous cell 'lMHR_Xmzcc5e')
communities_check = list(community.greedy_modularity_communities(subgraph_clean))

print(f"Number of communities detected: {len(communities_check)}")
if len(communities_check) == 1:
    print("Only one community was detected. This is why all nodes show 'Group 0'.")
    print("This might happen if the graph is too small or too densely connected for the algorithm to find distinct groups.")
    print("Consider running community detection on a larger graph (e.g., G_full) or adjusting parameters.")

"""# Task
For each detected community, I will aggregate the categories of all authors within that community and identify the most frequent categories to determine a descriptive name for the group. Then, I will modify the `plot_community_bridges_network` function to use these newly generated descriptive names for each community in the node tooltips, replacing the generic 'Group X' labels. Finally, I will verify that the community names in the plot tooltips are now descriptive and reflect the main research areas of the authors within each group, and confirm with the user that the visualization is more informative.

## Analyze Community Categories

### Subtask:
For each detected community, aggregate the categories of all authors within that community and identify the most frequent categories to determine a descriptive name for the group.

**Reasoning**:
The subtask requires iterating through each detected community, aggregating categories for all authors within it, identifying the most frequent categories, and generating a descriptive name for each community. This code block implements these steps as per the instructions.
"""

import collections

community_names = {}

for i, community_authors in enumerate(communities_check):
    all_community_categories = []
    for author in community_authors:
        # Retrieve categories, defaulting to an empty set if author not found
        author_categories = author_to_categories_map.get(author, set())
        all_community_categories.extend(list(author_categories))

    # Count frequency of each category
    category_counts = collections.Counter(all_community_categories)

    # Determine the most frequent category (or top 2 if available)
    if category_counts:
        most_common = category_counts.most_common(2)
        if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:
            # If top two categories have the same frequency, use both
            name = f"Primary Fields: {most_common[0][0]} & {most_common[1][0]}"
        else:
            name = f"Primary Field: {most_common[0][0]}"
    else:
        name = "Diverse/Undefined Field"

    community_names[i] = name

print("Community Descriptive Names:")
for comm_id, name in community_names.items():
    print(f"Community {comm_id}: {name}")

"""### Subtask:
Modify the `plot_community_bridges_network` function to use these newly generated descriptive names for each community in the node tooltips, replacing the generic 'Group X' labels. Finally, verify that the community names in the plot tooltips are now descriptive and reflect the main research areas of the authors within each group, and confirm with the user that the visualization is more informative.

#### Instructions
1. Locate the existing `plot_community_bridges_network` function definition.
2. Inside the `plot_community_bridges_network` function, modify the part where the `title_text` for each node is constructed. Specifically, replace `Group: {comm_id}` with `Community: {community_names.get(comm_id, 'Undefined Community')}`. Ensure the `community_names` dictionary is accessible within the function (it's a global variable in this notebook context).
3. Re-run the modified `plot_community_bridges_network` function with `subgraph_clean` as input.
4. Examine the generated HTML plot. Hover over different nodes to verify that the tooltips now display the descriptive community names instead of just `Group 0`, `Group 1`, etc.
5. Provide a confirmation that the visualization is more informative as requested.

**Reasoning**:
The previous step generated descriptive names for each community. This step involves modifying the `plot_community_bridges_network` function to use these descriptive names in the node tooltips, replacing the generic community IDs. This will make the network visualization more informative as per the subtask's requirement.
"""

from pyvis.network import Network
from networkx.algorithms import community
from IPython.display import display, HTML
import json

def plot_community_bridges_network(G, communities_detected, community_names_map, filename='community_bridge_network.html'):
    """
    Plots an interactive network with community detection and bridge author highlighting.
    Takes the graph, pre-detected communities, and descriptive community names as input.
    """
    # 1. Use pre-detected communities to build node_community_map
    # print("Detecting academic community structure...") # No longer needed, as communities are pre-detected
    communities = communities_detected

    # Print community sizes for clarity
    print(f"Detected {len(communities)} communities.")
    for i, comm in enumerate(communities):
        print(f"  Community {i}: {len(comm)} nodes")

    node_community_map = {}
    for i, comm in enumerate(communities):
        for node in comm:
            node_community_map[node] = i

    # 2. Identify Bridge Nodes
    bridge_nodes = []

    for node in G.nodes():
        my_comm = node_community_map.get(node)
        neighbors = list(G.neighbors(node))
        if not neighbors: continue

        foreign_neighbors = sum(1 for v in neighbors if node_community_map.get(v) != my_comm)

        if foreign_neighbors > 0:
            bridge_nodes.append(node)

    # 3. Set up PyVis visualization
    net = Network(notebook=True, height="650px", width="100%", bgcolor="#111111", font_color="white", cdn_resources='in_line')

    for node in G.nodes():
        comm_id = node_community_map.get(node)
        is_bridge = node in bridge_nodes

        node_color_config = None
        node_shape = 'dot'
        node_size = 15
        node_border_width = 1

        author_cats = sorted(list(author_to_categories_map.get(node, set())))
        categories_str = ", ".join(author_cats) if author_cats else "N/A"

        descriptive_community_name = community_names_map.get(comm_id, 'Undefined Community')

        title_text = f"Author: {node}\nCommunity: {descriptive_community_name}\nCategories: {categories_str}"
        if is_bridge:
            title_text += "\n(Cross-domain Bridge!)"

        if is_bridge:
            node_shape = 'diamond'
            node_size = 25
            node_border_width = 3
            node_color_config = {
                'border': '#FF0000',
                'highlight': {'border': '#FFD700'}
            }

        net.add_node(
            node,
            label=node,
            group=comm_id,
            title=title_text,
            font={'size': 14, 'color': 'white'},
            shape=node_shape,
            size=node_size,
            borderWidth=node_border_width,
            color=node_color_config
        )

    for source, target, data in G.edges(data=True):
        net.add_edge(source, target, value=data['weight'], color='#444444')

    # 4. Physics simulation settings
    physics_options = {
      "physics": {
        "solver": "forceAtlas2Based",
        "forceAtlas2Based": {
          "gravitationalConstant": -40000,
          "centralGravity": 0.2,
          "springLength": 120,
          "springConstant": 0.05,
          "avoidOverlap": 0.5
        },
        "stabilization": {
          "iterations": 3000,
          "fit": True,
          "enabled": True
        }
      }
    }
    net.set_options(json.dumps(physics_options))

    # 5. Save and display
    net.save_graph(filename)
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            display(HTML(f.read()))
    except Exception as e:
        print(f"Error: {e}")

# Execute the modified function
print(f"Analyzing network with descriptive community names... (Total {len(subgraph_clean.nodes())} nodes)")
plot_community_bridges_network(subgraph_clean, communities_check, community_names)

print("The interactive network plot has been updated. Please hover over the nodes in the generated plot to see the descriptive community names in the tooltips. This makes the visualization more informative by clearly indicating the primary research focus of each group.")